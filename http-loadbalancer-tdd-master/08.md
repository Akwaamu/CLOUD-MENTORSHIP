# Open Source Load Balancers

Chapter eight looks at two popular load balancers: Nginx and HAProxy. We'll look at what features they support and how they've evolved over the years to the state that they're currently in. Finally, we'll touch on what the future holds for load balancers.

## A Brief History of Open Source Load Balancers

In the late 1990s, as technology was booming, many companies operating at scale implemented their own load balancers much like we did in the previous chapters as the demand for specific applications surpassed the capabilities of individual servers. There were also a number of propriety hardware and software load balancers on the market. On the open source side of things, the [Apache HTTP Server](https://httpd.apache.org/) load balancer was coupled with Linux, MySQL, and PHP to create the LAMP stack. This application stack grew quickly, so the Apache Server became a highly adopted technology.

After the dot-com bust, the LAMP era began to declined opening the door to different application stacks implemented in Python, Ruby, and Java like Django and Rails. With this came new load balancers like Nginx and HAProxy.

## Nginx

[Nginx](https://www.nginx.com/) is a highly-performant load balancer, reverse proxy, web server, and more. All of the features we implemented in this course are present in Nginx. It provides a versatile configuration file that allows a wealth of options. Let’s take a look at some quick examples of what we implemented in previous chapters.

```
# nginx configuration
server {
  server_name sub1.example.com;
  location / {
    proxy_pass http://127.0.0.1:xxxx;
  }
}
server {
  server_name sub2.example.com;
  location / {
    proxy_pass http://127.0.0.1:xxxx;
  }
}

# upstreams
http {
  upstream backend {
    zone backend 64k;
    server backend1.example.com;
    server backend2.example.com;
    server backend3.example.com;
    server backend4.example.com;
  }
}

# health checks
location / {
    proxy_pass http://backend;
    health_check uri=/some/path;
}

# adding and Removing headers
add_header MyHeader Value;
proxy_hide_header MyHeader;

# modifying http query strings
location = /oneapi {
  set $args $args&apiKey=newparam;
  proxy_pass https://localhost:8080;
}

# rewriting urls
server {
  # ...
  rewrite ^(/download/.)/media/(\w+)\.?.$ $1/mp3/$2.mp3 last;
  rewrite ^(/download/.)/audio/(\w+)\.?.$ $1/mp3/$2.ra  last;
  return  403;
  # ...
}

# ip hash load balancing
upstream backend {
  ip_hash;
  server backend1.example.com;
  server backend2.example.com;
}

# weighted load balancing
upstream backend {
  server backend1.example.com weight=5;
  server backend2.example.com;
  server 192.0.0.1 backup;
}

# denying ips
location / {
  deny    10.192.0.1;
  allow   10.192.0.2;
  # drop everyone else
  deny    all;
}

# denying requests based on headers
location /api {
  proxy_http_version 1.1;
  if ($http_myheader != "WhatIWant") {
    return 401;
  }
  proxy_pass http://app:3000/;
}
```

The config above defines host-based routing in order to reverse proxy to a backend server. Similarly, it creates an `upstream` resource that takes a list of servers that Nginx load balances against. Health checks are supported as background processes. HTTP request manipulation is supported along with intelligent firewall rules.

## HAProxy

[HAProxy](http://www.haproxy.org/) is a reliable, high performance HTTP/TCP server. It's written in C. While its core is open source there's also a premium enterprise version. Similar to the Nginx section, let's look at a sample configuration file to see how it handles similar features:

```
frontend www
    bind :80
    acl bad_ip src 10.192.0.1
    tcp-request connection reject if bad_ip

    acl host_mango hdr(host) -i ilovemango.com
    acl host_milkshakes hdr(host) -i milkshakes.com

    monitor-uri /healthcheck

    reqrep ^([^\ :])\ /foo/(.)     \1\ /bar/\2

    use_backend mango_cluster if host_mango
    use_backend milkshake_cluster if host_milkshakes

backend mango_cluster
    balance leastconn
    option httpclose
    option forwardfor
    server node1 10.0.0.1:8080 check
    server node2 10.0.0.2:8080 check
    server node3 10.0.0.3:8080 check

backend milkshake_cluster
    balance leastconn
    option httpclose
    option forwardfor
    server node1 10.0.0.4:8080 check
    server node2 10.0.0.5:8080 check
    server node3 10.0.0.6:8080 check
```

HAProxy has "frontend" and "backend" concepts. The frontends are used to define which ports the HAProxy process binds to and supports access control lists (ACLs) used to route the request to the correct backends based on the HTTP request. Backends provide the ability to define algorithms and diverse health checks along manipulating the HTTP request.

## Load Balancing in the Container Ecosystem

In this section, we'll touch on what role load balancers play within container orchestration tools such as Kubernetes. Load balancing in a microservice, containerized world is different. Along with the main load balancer, you handle some of the load balancing features at the individual service-level. Kubernetes supports two types of resources -- Service and Ingress. A Service resource acts like a Layer 4 load balancer where it points to a set of Pods.

For example:

```yaml
# deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mango
  labels:
    app: mango
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mango
  template:
    metadata:
      labels:
        app: mango
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        # health check
        livenessProbe:
          httpGet:
            path: /healthz
            port: 80
            httpHeaders:
            - name: Host
              value: www.mango.com
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apple
  labels:
    app: apple
spec:
  replicas: 3
  selector:
    matchLabels:
      app: apple
  template:
    metadata:
      labels:
        app: apple
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        # health check
        livenessProbe:
          httpGet:
            path: /healthz
            port: 80
            httpHeaders:
            - name: Host
              value: www.apple.com
          initialDelaySeconds: 5
          periodSeconds: 5
```

The above will spin up the Nginx Pods for each deployment that listen on port 80. Healthchecks are done at the Pod level. Pods will be marked as down if they don't meet the requirements declared.

One can create a Service as follows:

```yaml
# service.yaml

apiVersion: v1
kind: Service
metadata:
  name: mango-service
spec:
  selector:
    app: mango
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: apple-service
spec:
  selector:
    app: apple
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9375
```

Once created, the Service is accessible from only within the Kubernetes cluster. Notice the `selector` which determines which set of Pods the Service points to. This way we only route traffic to only a set of Pods.

To expose the Service to outside of the cluster we need to create an Ingress Resource as follows:

```yaml
# ingress.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotatons:
    # Add Custom headers, remove headers, set rewrite rules, load balancing algorithm, firewall
    nginx.ingress.kubernetes.io/load-balance: round_robin
    nginx.ingress.kubernetes.io/whitelist-source-range: "10.0.0.0/24"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "SampleHeader: Test"
      proxy_hide_header MyHeader;

spec:
  rules:
  - host: www.mango.com # host based
    http:
      paths:
      - path: /test/?(.*) # path based routing with rewrite
        backend:
          serviceName: mango
          servicePort: 80
  - host: www.apple.com
    http:
      paths:
      - path: /anotherpath  # path based routing
        backend:
          serviceName: apple
          servicePort: 80
```

This creates a Layer 7 load balancer with Nginx Ingress, an ingress controller.

Now what does this have to do with Nginx and HAProxy? Kubernetes supports what’s known as an ingress controller. Kubernetes does not have native capabilities of exposing ingresses, rather it supports for example [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx) and [https://github.com/haproxytech/kubernetes-ingress](https://github.com/haproxytech/kubernetes-ingress). These ingress controllers are tightly coupled with the Kubernetes API and take out all the manual configuration steps typically found in traditional load balancers.

## Upcoming Load Balancers

Software is constantly improving and evolving. As with Nginx and HAProxy, they were created to solve real world problems faced in the early 200s and have evolved into the container ecosystem. The next generation of load balancers / reverse proxies to watch out for are:

- [Traefik](https://github.com/containous/traefik)
- [Envoy](https://github.com/envoyproxy/envoy)
- [Caddy](https://caddyserver.com/)

The above is just a small list in comparison to what's out there.
